{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N5cm2Jday0R-",
        "eKT0S5W0EiG2",
        "kRAyUG_eGTWJ",
        "QwM5NW-yJKmu",
        "LWWgq53mQRXi"
      ],
      "mount_file_id": "1_VlqQ_2JrsAZHQyQlm2Z6DhwoPhmaaq1",
      "authorship_tag": "ABX9TyP2Gl8UevdS2zJSvuqPlM7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnsocf/CSCI_3202_HW_5/blob/master/hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXZFc_qByck"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xpBaNNgpY7M"
      },
      "source": [
        "import os, sys\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2 as cv\n",
        "import matplotlib.patches as patches\n",
        "import statistics\n",
        "import json\n",
        "\n",
        "#sci kit learn packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# keras packages\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.experimental.preprocessing import CenterCrop\n",
        "\n",
        "# additional packages\n",
        "from google.colab import files\n",
        "from time import time\n",
        "from PIL import Image\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPGMlqNgzZn7"
      },
      "source": [
        "# Import Competition Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqowS4mWEIEz",
        "outputId": "bc34611b-b90d-49cf-f27e-ca38dd2dfe00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DM5fROBJu_C"
      },
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "!pip install --upgrade kaggle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsvpDJRwPBMn"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFb9uvOtJyp8",
        "outputId": "0f295794-b3e8-4100-f6fc-0c7de81387b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir .kaggle"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5E51iDZTkhp",
        "outputId": "3e6309e2-23bf-4d8b-dfac-7a7a25da9f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir .kaggle\n",
        "!ls -a"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘.kaggle’: File exists\n",
            ".\t histopathologic-cancer-detection.zip  test\t      train_labels.csv\n",
            "..\t .kaggle\t\t\t       test_cropped\n",
            ".config  sample_data\t\t\t       train\n",
            "drive\t sample_submission.csv\t\t       train_cropped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhbEy5CMJ79-"
      },
      "source": [
        "token = {\"username\":\"catnippsunn\",\"key\":\"d10a25e54993332212ca4e9f6a8dece2\"}\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsExGfozgR8Q",
        "outputId": "92fc060a-3796-4281-d35f-e1f99d760e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir .kaggle\n",
        "!ls -a"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘.kaggle’: File exists\n",
            ".\t histopathologic-cancer-detection.zip  test\t      train_labels.csv\n",
            "..\t .kaggle\t\t\t       test_cropped\n",
            ".config  sample_data\t\t\t       train\n",
            "drive\t sample_submission.csv\t\t       train_cropped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvD8ja0xRtBG"
      },
      "source": [
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLe252lQKGqC"
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CRWdljEKKhS",
        "outputId": "d50cdc2d-aef2-47ab-bb2c-f25ece9d5a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle config set -n path -v{/content}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfLn_bziKQR0"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G029oeNPOJ4m"
      },
      "source": [
        "# root_path = 'drive/My Drive/kaggle_dataset/'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z66lEY72Gfwh",
        "outputId": "a83b8b1f-8235-4518-8560-22227d116fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle competitions download -c histopathologic-cancer-detection -p /content"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "histopathologic-cancer-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlIJ8AKVLH7k",
        "outputId": "87923f3f-a123-4d05-b9ed-760abccdd7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip \\*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  histopathologic-cancer-detection.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIEXZT3zmDR"
      },
      "source": [
        "# Set up Data Path Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTGliOFwbhyD"
      },
      "source": [
        "# set useful paths to image and label directories for succinct use in the notebook\n",
        "model_path='.'\n",
        "path='/content/'\n",
        "train_folder='{}train'.format(path)\n",
        "test_folder='{}test'.format(path)\n",
        "train_folder_cr='{}train_cropped'.format(path)\n",
        "test_folder_cr='{}test_cropped'.format(path)\n",
        "train_label='{}train_labels.csv'.format(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtaJoIWxz2y8"
      },
      "source": [
        "Set up variables based on defaults for data directory paths and shapes.  This allows for easy configuration in the model building fitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZxlWUKLUGFG"
      },
      "source": [
        "directory_for_training = train_folder\n",
        "directory_for_testing = test_folder\n",
        "data_shape = (96, 96, 3)\n",
        "dims = (data_shape[0], data_shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVaghnjDzvrH"
      },
      "source": [
        "# configs used in model training and fit with cropped images.\n",
        "directory_for_training_cr = train_folder_cr\n",
        "directory_for_testing_cr = test_folder_cr\n",
        "data_shape_cr = (32, 32)\n",
        "dims_cr = (data_shape[0], data_shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5cm2Jday0R-"
      },
      "source": [
        "# Add an augmented copy of the dataset\n",
        "## Add an additional Directory containing a copy of the Images with a Centered 32 x 32 Crop\n",
        "\n",
        "This workbook will explore image cropping as an alternate option when providing data to train the model.  To do this each image is loaded, cropped, then saved with the same name to a new directory specific for croppped images.  A parallel model will train against one with the original images for analysis and comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JatCA3nx8rlE"
      },
      "source": [
        "# add new directories in which to store cropped images.\n",
        "!mkdir train_cropped\n",
        "!mkdir test_cropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJ5U4Bn6uwu"
      },
      "source": [
        "# cropping images to middle 32 x 32 section where tumor would lie. (from known (96,96) height/width.)\n",
        "# this test case involved setting image sizes to 32 x 32 instead of 96 x 96 in the keras model\n",
        "# and updating the directory to use the test _res directories.\n",
        "\n",
        "start_width, start_height = (96, 96)\n",
        "new_width, new_height = (32, 32)\n",
        "left_coord = start_width/4\n",
        "top_coord = start_height/4\n",
        "right_coord = 3 * start_width/4\n",
        "bottom_coord = 3 * start_height/4\n",
        "\n",
        "# crop function, iterates through each item in the directory.\n",
        "# uses python image module PIL.  ref: https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
        "# crops each image and saves it in a directory\n",
        "def crop_images(dirs, start_path, dest_path):\n",
        "    for item in dirs:\n",
        "        # if os.path.isfile(start_path+ '/' + item):\n",
        "        im = Image.open(start_path+'/' + item)\n",
        "        # splits text into root path and file extension (f and e)\n",
        "        # f, e = os.path.splitext(start_path+'/' + item)\n",
        "        # f_1, e_1 = os.path.splitext(dest_path+'/' + item)\n",
        "        imCrop = im.crop((left_coord, top_coord, right_coord, bottom_coord))\n",
        "        #imResize = imCrop.resize((96, 96))\n",
        "        imCrop.save(dest_path + '/' + item)\n",
        "\n",
        "# get directory path for test images\n",
        "dirs_test = os.listdir(test_folder)\n",
        "# \n",
        "crop_images(dirs_test, test_folder, test_folder_cr)\n",
        "# repeat for training images.\n",
        "dirs_train = os.listdir(train_folder)\n",
        "crop_images(dirs_train, train_folder, train_folder_cr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWTS8u_FO-7"
      },
      "source": [
        "##**Set up Data Viz functionality.**\n",
        "\n",
        "#code for displaying Data Viz:\n",
        "\n",
        "1.  Histogram is defined for data investigation.\n",
        "\n",
        "2.  Sample images are pulled out to display visual differences in images positive for cancerous tumor and those negative for a cancerous tumor.  This may give further idea of the visual distinction between classes.  The insight can be used to better understand correlation between feature size and CNN model architecture.\n",
        "Label data has an image number associated with positive (1) and negative (0) samples.  The ID associated with the sample in each label data item will map to an image in the train folders.  This image can be read and stored for printing to the screen.\n",
        "\n",
        "3.  Using the RGB channel, determine the mean colors and most seen colors per image.  Plot these out.  This may show a consistency in the colors found in the data in positive and negative samples, and should be considered relevant to overfitting in analysis.  Since training on RGB color scale could be a feature that could skew the results and be unreplicatable in potential future samples used with the model to determine predictions.  (i.e. purple tinged samples may be more representative of positive samples, while pink may be representative of negative samples in the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdMAeY5iLTu4"
      },
      "source": [
        "1.  Histogram Display Code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD5xieBVpd8e"
      },
      "source": [
        "# store function for data viz distribution histogram, used below.\n",
        "def define_histogram(data, range, title, x_axis_label, y_axis_label, ax, graphType=\"\"):\n",
        "    colors = ['r', 'g', 'b']\n",
        "    if range:\n",
        "        counts, bins, p = ax.hist(data, range = range, bins=2, label=\"yellow\", edgecolor='white', linewidth=1)\n",
        "        p[0].set_facecolor('b')\n",
        "        p[1].set_facecolor('y')\n",
        "        ax.set_xticks(bins)\n",
        "        ax.annotate('Negative', xy=(.5, 0), xycoords=('data', 'axes fraction'),\n",
        "        xytext=(0, -18), textcoords='offset points', va='top', ha='center')\n",
        "        ax.annotate('Positive', xy=(1.5, 0), xycoords=('data', 'axes fraction'),\n",
        "        xytext=(0, -18), textcoords='offset points', va='top', ha='center')\n",
        "    else:\n",
        "        counts, bins, path = ax.hist(data, range=(0, 255), label=\"yellow\", bins=4, edgecolor='white', linewidth=2)\n",
        "        for idx, x in enumerate(path):\n",
        "            for y in x:\n",
        "                y.set_facecolor(colors[idx])\n",
        "        ax.legend((path[0], path[1], path[2]),(\"R\",\"G\",\"B\"))\n",
        "        \n",
        "        ax.set_xticks(bins)\n",
        "        ax.title.set_text(graphType)\n",
        "        ax.xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))\n",
        "    \n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(x_axis_label)\n",
        "    plt.ylabel(y_axis_label)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3yTt5LuLXg-"
      },
      "source": [
        "2.  Sample image procurement code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh8jFJiQckTw"
      },
      "source": [
        "# loc gets rows from a pandas df given a certain set of conditions.\n",
        "positive_image_samples = label_data_df.loc[label_data_df['label'] == 1].sample(6)\n",
        "negative_image_samples = label_data_df.loc[label_data_df['label'] == 0].sample(6)\n",
        "positive_image_sample_array = []\n",
        "negative_image_sample_array = []\n",
        "for sample in positive_image_samples['id']:\n",
        "    path = os.path.join(train_folder, sample+'.tif')\n",
        "    # cv2 open cv is a library for computer vision which provides an api for\n",
        "    # reading an image file into an 2D array.\n",
        "    img = cv.imread(path)\n",
        "    # RAM space allow for storing a few images\n",
        "    positive_image_sample_array.append(img)\n",
        "        \n",
        "# repeat for negative samples\n",
        "for sample in negative_image_samples['id']:\n",
        "    path = os.path.join(train_folder, sample+'.tif')\n",
        "    img = cv.imread(path)\n",
        "    negative_image_sample_array.append(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ_Mne_aLbAC"
      },
      "source": [
        "3.  Image display code for sample images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDZlyUIvIqFc"
      },
      "source": [
        "def show_image_samples(images, title, axis, r):\n",
        "  for i,img in enumerate(images, 0):\n",
        "    axis[r,i].imshow(img)\n",
        "    rect = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='g',facecolor='none', linestyle=':', capstyle='round')\n",
        "    axis[r,i].add_patch(rect)\n",
        "  axis[r,0].set_ylabel(title, size='large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKTzxwJNLfDi"
      },
      "source": [
        "4.  Average and Prominent Color Depictions.\n",
        "The code credit reference for this snippet which was adapted for the specifics of this purpose is from:\n",
        "https://stackoverflow.com/questions/43111029/how-to-find-the-average-colour-of-an-image-in-python-with-opencv.  \n",
        "Explanations of the code are included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWd2DhYwKTSX"
      },
      "source": [
        "avg_colors_pos = []\n",
        "avg_colors_neg = []\n",
        "\n",
        "\n",
        "def show_colors(color_image, img_type, plot_title):\n",
        "    average = color_image.mean(axis=0).mean(axis=0) # get the mean of each chromatic channel\n",
        "    pixels = np.float32(color_image.reshape(-1, 3))\n",
        "\n",
        "    n_colors = 5\n",
        "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 200, .1)\n",
        "    flags = cv.KMEANS_RANDOM_CENTERS\n",
        "\n",
        "    _, labels, palette = cv.kmeans(pixels, n_colors, None, criteria, 10, flags)\n",
        "    _, counts = np.unique(labels, return_counts=True)\n",
        "    dominant = palette[np.argmax(counts)]\n",
        "    \n",
        "    avg_patch = np.ones(shape=color_image.shape, dtype=np.uint8)*np.uint8(average)\n",
        "    if img_type == 'pos':\n",
        "        avg_colors_pos.append(average)\n",
        "    else:\n",
        "        avg_colors_neg.append(average)\n",
        "\n",
        "    indices = np.argsort(counts)[::-1]   \n",
        "    freqs = np.cumsum(np.hstack([[0], counts[indices]/float(counts.sum())]))\n",
        "    rows = np.int_(color_image[0].shape[0]*freqs)\n",
        "\n",
        "    dom_patch = np.zeros(shape=color_image.shape, dtype=np.uint8)\n",
        "    for i in range(len(rows) - 1):\n",
        "        dom_patch[rows[i]:rows[i + 1], :, :] += np.uint8(palette[indices[i]])\n",
        "\n",
        "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(12,6))\n",
        "    if plot_title:\n",
        "      fig.suptitle(plot_title,fontsize=20)\n",
        "    ax0.imshow(avg_patch)\n",
        "    ax0.set_title('Average color')\n",
        "    ax0.axis('off')\n",
        "    ax1.imshow(dom_patch)\n",
        "    ax1.set_title('Dominant colors')\n",
        "    ax1.axis('off')\n",
        "    plt.show(fig)\n",
        "    \n",
        "def show_image_colors(data, title, ax):\n",
        "    ax.imshow(data)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKT0S5W0EiG2"
      },
      "source": [
        "# Data Exploration\n",
        "## image labels\n",
        "\n",
        "Investigate data distribution of positive and negative samples within data.  It is evident that out of over 220K samples, about 60% of the samples are negative and 40% are positive.  The lack of consistent samples in each binary category may leave the CNN susceptable for overfitting, so additional measures to prevent this will be addressed in data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuSp9Vq3pZhz"
      },
      "source": [
        "label_data_df = pd.read_csv(train_label)\n",
        "print(label_data_df['label'].value_counts(normalize=True))\n",
        "print(label_data_df.head())\n",
        "label_data_df.describe()\n",
        "print('')\n",
        "print('Number of training images : ', len(label_data_df))\n",
        "pos = round(sum(label_data_df['label'].values)/len(label_data_df), 2) * 100\n",
        "neg = round((100 - pos), 2)\n",
        "print('Ratio of positive to negative samples: ', \"%d:%d\" % (pos, neg))\n",
        "test_img = plt.imread(\"/content/train/\"+label_data_df.iloc[0]['id']+'.tif')\n",
        "print('Images shape (M, N, d) where d = RGB value channels', test_img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRAyUG_eGTWJ"
      },
      "source": [
        "# Data Viz:  \n",
        "## Histogram Distribution\n",
        "\n",
        "A data vis using matplot library to show a histogram of the positive and negative sample ration within the dataset depicts in further detail the binary ratio of samples mentioned from the descriptive data set stats previously.  This visually depicts the skew'd ratio tending towards more negative samples in the dataset, which could lead towards false negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuSkVUTHvZg"
      },
      "source": [
        "# utilize binary histogram function above.\n",
        "fig, ax = plt.subplots()    \n",
        "define_histogram(label_data_df.label, (0, 2), 'Distribution of Positive and Negative Samples', 'Label', 'Number of samples', ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwM5NW-yJKmu"
      },
      "source": [
        "## Image Samples\n",
        "\n",
        "The following plots show the visual difference in positive and negative image samples, looking at the original 96 x 96 image as well as a center cropped version which isolates the patch where the cancerous tumor can be found that defines an image positive for the presence of cancer.  These depictions show just how feasible it is to determine this cancer given a naked eye.  The cancerous cell can be as small as 1px evident.  The model will be trained on both cropped and uncropped images to deduce if having more of the presence of outerlying cell data will add features to the CNN that become important.  If the cropped version is more accurate it will be deducable that training on the immediately relevant data is of value in fitting a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q1Y-M6NYEdc"
      },
      "source": [
        "fig, ax0 = plt.subplots(2,6,figsize=(20,8))\n",
        "fig.suptitle('Sample Positive And Negative Images',fontsize=20)\n",
        "show_image_samples(positive_image_sample_array, 'Positive samples', ax0, 0)\n",
        "show_image_samples(negative_image_sample_array, 'Negative samples', ax0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSfInCQYNADk"
      },
      "source": [
        "## Cropped Image Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_UH_6fUY9XD"
      },
      "source": [
        "cropper = CenterCrop(height=32, width=32)\n",
        "fig.suptitle('Sample 32 x 32 Center Cropped Positive And Negative Images',fontsize=20)\n",
        "output_data_pos = cropper(positive_image_sample_array)\n",
        "output_data_neg = cropper(negative_image_sample_array)\n",
        "fig, ax0 = plt.subplots(2,6,figsize=(20,8))\n",
        "show_image_samples(output_data_pos, 'Positive samples', ax0, 0)\n",
        "show_image_samples(output_data_neg, 'Negative samples', ax0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix0IcQ5zNGDq"
      },
      "source": [
        "## Color Trends in Image Samples\n",
        "Displaying Average and Dominent Colors in Positive and Negative Samples.  There are immediately evident consistencies but not direct parallels within each binary classification group.  This visualization assumes the image subsamples are representative of the whole since they have been selected randomly.  The image subsamples are consistent with those depicted in the sample images, above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tklV9sR3lh3y"
      },
      "source": [
        "show_colors(positive_image_sample_array[0], 'pos', 'Positive Image Samples')\n",
        "show_colors(positive_image_sample_array[1], 'pos', '')\n",
        "show_colors(positive_image_sample_array[2], 'pos', '')\n",
        "show_colors(positive_image_sample_array[len(positive_image_sample_array)-1], 'pos', '')\n",
        "show_colors(negative_image_sample_array[0], 'neg', 'Negative Image Samples')\n",
        "show_colors(negative_image_sample_array[1], 'neg', '')\n",
        "show_colors(negative_image_sample_array[2], 'neg', '')\n",
        "show_colors(negative_image_sample_array[len(negative_image_sample_array)-1], 'neg', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2eoP88QL0x"
      },
      "source": [
        "# Data Cleaning and Augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWWgq53mQRXi"
      },
      "source": [
        "## Labels Dataframe Preprocessing\n",
        "\n",
        "The Binary Class Mode in Keras preprocessing requires classes to be Strings.  This necessitates a translation from the stored data set which has classes stored as 0 and 1 format to represent negative and positive images, respectively.\n",
        "Ref: https://keras.io/api/preprocessing/image/\n",
        "\n",
        "The image ID in the label data set needs to also be adjusted to include the file extension since the mapping of ID will be used directly look up the image in the training and verification directories subsequent steps to set up the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwifzpfE5l0O"
      },
      "source": [
        "# update labels since `class_mode='binary' requires classes to be strings.\n",
        "label_data_df['label'] = np.where(label_data_df['label'] == 0, 'neg','pos')\n",
        "# update image IDs to include the .tif file format for proper processing in\n",
        "# subsequent steps.\n",
        "label_data_df['id'] = label_data_df['id'] + '.tif'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RYjvMFxRN_s"
      },
      "source": [
        "## Image Preprocessing\n",
        "\n",
        "Keras, the API for the popular open-source Google library for Machine Learning, Tensorflow, provides an Image Data Generator in the preprocessing image library which creates augmentations of a data set, to reduce overfitting.  \n",
        "\n",
        "Overfitting can be noted by accuracy and solid metrics from training however less optimal results on testing against images not used in training.  \n",
        "\n",
        "Since features could be found not representative of the desired features for training, augmenting the dataset to include variations on dataset images prior to model training can be adventageous to avoid overfitting.  The potential property settings for data autmentation I considered and experimented with this for this CNN are listed below.  Adding too much data augmentation to this model might prove to be adverse, and is a consideration in tuning the approach.  The best use for data augmentation is on a small dataset however the dataset is already quite robust.\n",
        "\n",
        "### References\n",
        "\n",
        "Definitions from docs, ref: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
        "\n",
        "Conceptual and applicative ref: https://towardsdatascience.com/image-augmentation-for-deep-learning-using-keras-and-histogram-equalization-9329f6ae5085\n",
        "\n",
        "## model properties:\n",
        "\n",
        "featurewise_center: Boolean. Set input mean to 0 over the dataset, feature-wise.\n",
        "\n",
        "samplewise_center: Boolean. Set each sample mean to 0.\n",
        "\n",
        "featurewise_std_normalization: Boolean. Divide inputs by std of the dataset, feature-wise.\n",
        "\n",
        "samplewise_std_normalization: Boolean. Divide each input by its std.\n",
        "\n",
        "rotation_range:Int. Degree range for random rotations.,\n",
        "\n",
        "width_shift_range:Float, 1-D array-like or int,\n",
        "\n",
        "height_shift_range:Float, 1-D array-like or int,\n",
        "\n",
        "brightness_range:Tuple or list of two floats. Range for picking a brightness shift value from.\n",
        "\n",
        "shear_range:Shear angle in counter-clockwise direction in degrees,\n",
        "\n",
        "zoom_range:Float or [lower, upper]. Range for random zoom\n",
        "\n",
        "channel_shift_range: Float. Range for random channel shifts.\n",
        "\n",
        "horizontal_flip: Boolean. Randomly flip inputs horizontally.,\n",
        "\n",
        "fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'                            \n",
        "\n",
        "vertical_flip: Boolean. Randomly flip inputs vertically.,\n",
        "\n",
        "brightness_range: Tuple or list of two floats. Range for picking a brightness shift value from,\n",
        "\n",
        "validation_split: Float. Fraction of images reserved for validation (strictly between 0 and 1),\n",
        "\n",
        "rescale: rescaling factor. normalise like: (x-x_min)/(x_max-x_min),\n",
        "\n",
        "preprocessing_function: function that will be applied on each input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-gfAw7LeBR5"
      },
      "source": [
        "datagen_training = ImageDataGenerator(\n",
        "        # normalize input x_scaled = (x-x_min)/(x_max-x_min)\n",
        "        rescale=1./255, #keep\n",
        "        validation_split=0.15, #keep\n",
        "        # rotation_range=10,\n",
        "        # shear_range=0.1, # do not keep.\n",
        "        # width_shift_range=0.1,\n",
        "        # height_shift_range=0.1,\n",
        "        zoom_range=0.01, #keep\n",
        "        horizontal_flip=True, #keep\n",
        "        # vertical_flip=True, # do not keep.\n",
        "        # preprocessing_function=myFunc, # maybe.  test over mult epoch.\n",
        ")\n",
        "  \n",
        "# there is no benefit in testing against augmented images.  TODO:  try it!\n",
        "datagen_testing = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbewflWoW2J1"
      },
      "source": [
        "# Data Preprocessing Using Image Data Generator Specifications\n",
        "\n",
        "To Preprocess the data before it is fit to the model, it is necessary to combine the image generator specifications configured above with the the dataframe mapping the labels to images, and the directory information pointing to the images for training and testing.  The proper set of augmented images using the Keras api can be built using the method flow_from_dataframe also available in the preprocessing image library.  With this method we set the data up to be training in the model by specifying parameters including batch size, type of classifier, target size, and point the model fitting function to the actual images.  This Keras API method has an expectation or 'contract' with the way it expects to configure images, which fits perfectly with the setup that we have been provided.\n",
        "\n",
        "flow_from_dataframe: is a special keras method to use data in a single folder that maps filenames of images with their class.  \n",
        "\n",
        "## References:\n",
        "https://keras.io/api/preprocessing/image/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Wgngmte9Qq"
      },
      "source": [
        "flow_training_params_config_original = {\n",
        "    # point preprocessing method to the image/label mapping df\n",
        "    'df': label_data_df,\n",
        "    # point preprocessign method to the image folder\n",
        "    'dir': train_folder,\n",
        "    # specify column for directory image lookup (class y given x)\n",
        "    'xcol': 'id',\n",
        "    # specify column class for classification (x given class y)\n",
        "    'ycol': 'label',\n",
        "    # name of this subset\n",
        "    'subset_name': 'training',\n",
        "    # target size will be the output image size.\n",
        "    'target_dims': dims,\n",
        "    # batch size for each iteration within an epoch\n",
        "    'batches': 64,\n",
        "    # the type of classifier\n",
        "    'mode': 'binary'\n",
        "    'shuffle': True,\n",
        "}\n",
        "flow_testing_params_config_original = {\n",
        "    'df': label_data_df,\n",
        "    'dir': test_folder,\n",
        "    'xcol': 'id',\n",
        "    'ycol': 'label',\n",
        "    'subset_name': 'validation',\n",
        "    'target_dims': dims,\n",
        "    'batches': 64,\n",
        "    'shuffle': False,\n",
        "    'mode': 'binary'\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itnzov3o81JX"
      },
      "source": [
        "(df, dir, xcol, ycol, subset_name, target_dims, batches, shuffle, mode) = flow_training_params_config_original\n",
        "generator_training_flow = datagen_training.flow_from_dataframe(\n",
        "                # point preprocessing method to the image/label mapping df\n",
        "                dataframe = df,\n",
        "                # point preprocessign method to the image folder\n",
        "                directory = dir,\n",
        "                # specify column for directory image lookup (class y given x)\n",
        "                x_col = xcol,\n",
        "                # specify column class for classification (x given class y)\n",
        "                y_col = ycol,\n",
        "                # name of this subset\n",
        "                subset = subset_name\n",
        "                # target size will be the output image size.\n",
        "                target_size = target_dims,\n",
        "                # batch size for each iteration within an epoch\n",
        "                batch_size = batches,\n",
        "                # the type of classifier\n",
        "                class_mode = mode\n",
        "                )\n",
        "\n",
        "(df, dir, xcol, ycol, subset_name, target_dims, batches, shuffle, mode) = flow_testing_params_config_original\n",
        "generator_testing_flow = datagen_testing.flow_from_dataframe(\n",
        "                dataframe = df,\n",
        "                directory = dir,\n",
        "                x_col = xcol,\n",
        "                y_col = ycol,\n",
        "                subset= subset_name,\n",
        "                target_size = target_dims,\n",
        "                batch_size = batches,\n",
        "                shuffle = shuffle,\n",
        "                class_mode = mode\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBreLMIz5JHy"
      },
      "source": [
        "# override configs to point directory to cropped images.\n",
        "flow_training_params_config_original.dir = train_folder_cr\n",
        "flow_testing_params_config_original.dir = test_folder_cr\n",
        "\n",
        "# repeat workflow above for cropped image generators.\n",
        "(df, dir, xcol, ycol, subset_name, target_dims, batches, shuffle, mode) = flow_training_params_config_original\n",
        "generator_training_flow_cr = datagen_training.flow_from_dataframe(\n",
        "                dataframe = df,\n",
        "                directory = dir,\n",
        "                x_col = xcol,\n",
        "                y_col = ycol,\n",
        "                subset = subset_name\n",
        "                target_size = target_dims,\n",
        "                batch_size = batches,\n",
        "                class_mode = mode\n",
        "                shuffle = shuffle\n",
        "                )\n",
        "\n",
        "(df, dir, xcol, ycol, subset_name, target_dims, batches, shuffle, mode) = flow_testing_params_config_original\n",
        "generator_testing_flow_cr = datagen_testing.flow_from_dataframe(\n",
        "                dataframe = df,\n",
        "                directory = dir,\n",
        "                x_col = xcol,\n",
        "                y_col = ycol,\n",
        "                subset= subset_name,\n",
        "                target_size = target_dims,\n",
        "                batch_size = batches,\n",
        "                shuffle = shuffle,\n",
        "                class_mode = mode\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJHn9imCcUcX"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "BIG TODO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhKGD6k6-aZx"
      },
      "source": [
        "single_layer_interface = {\n",
        "    'has_layer': False,\n",
        "    'layer_value': 0,\n",
        "    'layer_number': 0,\n",
        "    'layer_function': None,\n",
        "    'layer_shape': None\n",
        "}\n",
        "\n",
        "def add_relu_conv2D(model, nodes, number, shaped):\n",
        "  for i in range (0, number-1):\n",
        "    if shaped:\n",
        "      conv2D_layer = Conv2D(filters = nodes, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = shaped);\n",
        "    else:\n",
        "      conv2D_layer = Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    model.add(conv2D_layer)\n",
        "    return model\n",
        "\n",
        "def add_layer(model, dropout_config, pooling_config, conv_config):\n",
        "  if (conv.has_layer):\n",
        "    model = conv_config.layer_function(model, conv_config.layer_value, conv_config.layer_number, conv_config.layer_shaped)\n",
        "  if (dropout_config.has_layer):\n",
        "    model.add(Dropout(dropout_config.layer_value))\n",
        "  if (pooling_config.has_layer):\n",
        "    model.add(MaxPooling2D(pool_size = pooling_config.layer_value))\n",
        "  return model;\n",
        "\n",
        "output_config_interface = {\n",
        "    'density_value': 128,\n",
        "    'dropout_value': .3,\n",
        "}\n",
        "\n",
        "def add_output_layers(output_config):\n",
        "  # complete with a fully connected layer.\n",
        "  # Flatten connects convolution and dense layers.\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(output_config.density_value, activation='relu'))\n",
        "  model.add(Dropout(output_config.dropout_value))\n",
        "  # classification layer for output.\n",
        "  # Dense is a standard layer type used for an output layer.\n",
        "  # sigmoid activation is used for classification.  returns a value between 0 and 1.\n",
        "  # e^x/(e^x + 1)\n",
        "  # 1: a node which is the possible outcome.  This is the only option for the binary classifier\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIqtzrB9fDh6"
      },
      "source": [
        "conv_1 = {\n",
        "    'has_layer': True,\n",
        "    'layer_value': 16,\n",
        "    'layer_number': 3,\n",
        "    'layer_function': add_layer,\n",
        "    'layer_shape': data_shape,\n",
        "}\n",
        "\n",
        "dropout_1 = {\n",
        "    'has_layer': True,\n",
        "    'layer_value': .3,\n",
        "    'layer_number': 1,\n",
        "    'layer_function': None,\n",
        "    'layer_shape': None,\n",
        "}\n",
        "\n",
        "max_pooling_1 = {\n",
        "    'has_layer': True,\n",
        "    'layer_value': 3,\n",
        "    'layer_number': 1,\n",
        "    'layer_function': None,\n",
        "    'layer_shape': None,\n",
        "}\n",
        "\n",
        "n_a = {\n",
        "    'has_layer': False,\n",
        "    'layer_value': 0,\n",
        "    'layer_number': 0,\n",
        "    'layer_function': None,\n",
        "    'layer_shape': None,\n",
        "}\n",
        "\n",
        "output = {\n",
        "    'density_value': 128,\n",
        "    'dropout_value': .3,\n",
        "}\n",
        "\n",
        "conv_2 = conv_1\n",
        "conv_2.layer_value = 2 * conv_1.layer_value\n",
        "conv_2.layer_function = None\n",
        "\n",
        "conv_3 = conv_2\n",
        "conv_3.layer_value = 2 * conv_2.layer_value\n",
        "\n",
        "conv_4 = conv_3\n",
        "conv_4.layer_value = 2 * conv_3.layer_value\n",
        "\n",
        "# create the model\n",
        "model = Sequential()\n",
        "\n",
        "# architectural layers:  VGGnet:  https://towardsdatascience.com/image-detection-from-scratch-in-keras-f314872006c9\n",
        "# model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = data_shape))\n",
        "# model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(MaxPooling2D(pool_size = 3))\n",
        "model = add_layer(model, dropout_1, max_pooling_1, conv_1)\n",
        "\n",
        "# model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(MaxPooling2D(pool_size = 3))\n",
        "add_layer(model, dropout_1, max_pooling_1, conv_2)\n",
        "\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(MaxPooling2D(pool_size = 3))\n",
        "\n",
        "model = model = add_layer(model, dropout_1, max_pooling_1, conv_3)\n",
        "\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model = add_layer(model, dropout_1, max_pooling_1, conv_4)\n",
        "model = add_output_layers()\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov0fK7dzRzPk"
      },
      "source": [
        "# # alt architecture 1.  VGGnet.\n",
        "# # create the model\n",
        "# model = Sequential()\n",
        "\n",
        "# # architecture:  VGGnet:  https://towardsdatascience.com/image-detection-from-scratch-in-keras-f314872006c9\n",
        "# model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = data_shape))\n",
        "\n",
        "# model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "# # complete with a fully connected layer.\n",
        "# # Flatten connects convolution and dense layers.\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# # classification layer for output.\n",
        "# # Dense is a standard layer type used for an output layer.\n",
        "# # sigmoid activation is used for classification.  returns a value between 0 and 1.\n",
        "# # e^x/(e^x + 1)\n",
        "# # 1: a node which is the possible outcome.\n",
        "# model.add(Dense(1, activation = 'sigmoid'))\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IeV_bgyXK-m"
      },
      "source": [
        "# Code and Calculations for Metric Analysis.\n",
        "\n",
        "Decisions on the metrics to log in each epoch cycle will be specified in the subsequent model compilation step.  The metrics for loss and accuracy are provided in Keras, however these additional metrics on recall, precision, and F-1 value give us further insight into how the model is performing.\n",
        "\n",
        "Logging these metrics can be adventageous in determining appropriate architecture, tuning params, and making decisions on how to best preprocess data, given multiple trial runs with various configurations of these subcomponents in the CNN.\n",
        "\n",
        "TODO: determine optimal values for these for analysis.\n",
        "\n",
        "## Recall:  Calculates positive predictions out of all positive samples in the data\n",
        "recall = true positives / (true positives + false negatives)\n",
        "\n",
        "## Precision:  Calculates the number of positive samples that are actually positive.\n",
        "precision = true positives / (true positives + false positives)\n",
        "\n",
        "\n",
        "## F-1:  A single score that balances precision and recall values\n",
        "F1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "Reference: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX9MvAaWFvJt"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    # true positives are found by multiplying the number true times the percent predicted.\n",
        "    # K.clip clips the multiplication of items predicted and actually existing to between 0 and 1.\n",
        "    # K.round (TODO)\n",
        "    # K.sum (TODO)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stFMpQNeALf7"
      },
      "source": [
        "# ex. model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvTj0U-hcqIW"
      },
      "source": [
        "# Compile and Fit the Model\n",
        "\n",
        "optimizer controls the learning rate.  'adam' is an optimizer makes\n",
        "adjustments to the learning rate during training.  smaller learning\n",
        "rate may be more accurate at the cost of more time.\n",
        "https://faroit.com/keras-docs/0.2.0/optimizers/\n",
        "\n",
        "loss: binary_crossentropy.  \n",
        "   predicts loss between true lavels and predicted labels.\n",
        ".   lower score reflects accurate performance.\n",
        "https://keras.io/api/losses/\n",
        "\n",
        "metrics: logs whichever metrics desired as a printout during training.\n",
        "accuracy:  This frequency is ultimately returned as binary accuracy \n",
        "ref: https://keras.io/api/metrics/accuracy_metrics/#binaryaccuracy-class\n",
        "\n",
        "epoch:  Entire db is passed through the model.  # is relevant to diversity of data.\n",
        "batches:  Number of iterations of an epoch.  \n",
        "step size:  Total number in data set / batch size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOyxFr87dzdS"
      },
      "source": [
        "# Original Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDZYOu9qiwF4"
      },
      "source": [
        "# TODO (add metrics back in for precision, recall, f1)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# TODO (find additional reference for this setup.)\n",
        "STEP_SIZE_TRAINING=generator_training_flow.n//generator_training_flow.batch_size\n",
        "# Total number of images in the data set / batch size \n",
        "# (batch size is set in preprocessing api above).\n",
        "STEP_SIZE_TESTING=generator_testing_flow.n//generator_testing_flow.batch_size\n",
        "\n",
        "model.fit_generator(\n",
        "                generator_training_flow,\n",
        "                steps_per_epoch=STEP_SIZE_TRAINING,\n",
        "                epochs=20,\n",
        "                validation_data=generator_testing_flow,\n",
        "                validation_steps=STEP_SIZE_TESTING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER9weuSsT51h"
      },
      "source": [
        "ref on, 'fit or fit_generator':  https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVNtyz7nd3BQ"
      },
      "source": [
        "# Cropped Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyoGbE8td-n0"
      },
      "source": [
        "## Metric Analysis on Performance\n",
        "\n",
        "Big TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raCiyEcSkTxG"
      },
      "source": [
        "# max: 0.9179 on non cropped images.\n",
        "# .86/ 20 epoch with cropped images and standard.\n",
        "# .599/ 20 cropped and nonstandard."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUyv21ZgkCse"
      },
      "source": [
        "# metrics:  https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
        "# accuracy, precision, recall.\n",
        "# f1, log loss (specific for binary classification), auc\n",
        "\n",
        "# howto?  calc myself or use classificationReport from scikit\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# https://keras.io/api/metrics/accuracy_metrics/#binaryaccuracy-class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kYv2bEODxPS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}